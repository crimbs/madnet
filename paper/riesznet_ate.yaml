# See Appendix A.1. RieszNet Architecture and Training Details
fit_kwargs:
  # Fast training step
  - batch_size: 64
    learning_rate: 0.0001
    min_delta: 0.0
    num_epochs: 100
    optimizer: adamw # Adam with weight decay regularization
    patience: 2
    validation_size: 0.2
    verbose: false
    weight_decay: 0.001
  # Fine-tuning step
  - batch_size: 64
    learning_rate: 0.00001
    min_delta: 0.0
    num_epochs: 600
    optimizer: adamw
    patience: 40
    validation_size: 0.2
    verbose: false
    weight_decay: 0.001
init_kwargs:
  nonshared_depth: 2
  rr_weight: 0.1
  shared_depth: 4
  target_reg: 1.0
  width_size: 200
  binary: true
learner: RieszNet
